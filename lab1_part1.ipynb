{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c09cde9",
   "metadata": {},
   "source": [
    "# Lab 1: Building a Basic RAG System\n",
    "\n",
    "In this lab, we'll create a simple Retrieval Augmented Generation (RAG) system using PyTorch and Hugging Face models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c862768",
   "metadata": {},
   "source": [
    "## Setup\n",
    "First, let's import the necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7cffec9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5defa09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third-party imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4b49dbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available and set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4c7219",
   "metadata": {},
   "source": [
    "## Download and Load Language Model\n",
    "\n",
    "We'll use a pre-trained language model from Hugging Face for generating embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0de07300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We'll use the model: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "# Define which model to use - we'll use a small but effective model\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "print(f\"We'll use the model: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "41738165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded with vocabulary size: 30522\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer - this converts text to tokens the model can understand\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(f\"Tokenizer loaded with vocabulary size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c35d56bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: Hello, this is a sample text for our RAG system!\n",
      "Token IDs: [101, 7592, 1010, 2023, 2003, 1037, 7099, 3793, 2005, 2256, 17768, 2291, 999, 102]\n",
      "Decoded tokens: ['[CLS]', 'hello', ',', 'this', 'is', 'a', 'sample', 'text', 'for', 'our', 'rag', 'system', '!', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# Let's see how the tokenizer works with a simple example\n",
    "example_text = \"Hello, this is a sample text for our RAG system!\"\n",
    "tokens = tokenizer(example_text)\n",
    "print(\"Input text:\", example_text)\n",
    "print(\"Token IDs:\", tokens[\"input_ids\"])\n",
    "print(\"Decoded tokens:\", tokenizer.convert_ids_to_tokens(tokens[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fece451e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully with 22713216 parameters\n"
     ]
    }
   ],
   "source": [
    "# Now load the actual model\n",
    "model = AutoModel.from_pretrained(model_name).to(device)\n",
    "print(f\"Model loaded successfully with {sum(p.numel() for p in model.parameters())} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ddc01c",
   "metadata": {},
   "source": [
    "## Generate Embeddings\n",
    "\n",
    "Now we'll see how to generate embeddings for text. Embeddings are vector representations that capture semantic meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "75f57628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded input shape:\n",
      "  input_ids: torch.Size([1, 14])\n",
      "  token_type_ids: torch.Size([1, 14])\n",
      "  attention_mask: torch.Size([1, 14])\n"
     ]
    }
   ],
   "source": [
    "# First, prepare text for the model by tokenizing it\n",
    "text_to_embed = \"This is a sample text to demonstrate embedding generation.\"\n",
    "encoded_input = tokenizer(text_to_embed, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "print(\"Encoded input shape:\")\n",
    "for key, value in encoded_input.items():\n",
    "    print(f\"  {key}: {value.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5276e6e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output keys: odict_keys(['last_hidden_state', 'pooler_output'])\n",
      "Last hidden state shape: torch.Size([1, 14, 384])\n"
     ]
    }
   ],
   "source": [
    "# Pass the encoded input through the model\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "# Look at the model output\n",
    "print(\"Model output keys:\", model_output.keys())\n",
    "print(\"Last hidden state shape:\", model_output.last_hidden_state.shape)\n",
    "# This is a 3D tensor: [batch_size, sequence_length, hidden_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "72085266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get a single vector for the entire text, we'll use mean pooling\n",
    "# Since we're only processing a single sentence without batching,\n",
    "# we can simply take the mean of the token embeddings\n",
    "token_embeddings = model_output.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3b744e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final embedding shape: torch.Size([384])\n",
      "Embedding numpy shape: (384,)\n",
      "First 5 values: [-0.12805542 -0.17154984  0.18549927  0.11615918  0.05051429]\n"
     ]
    }
   ],
   "source": [
    "# Calculate mean across the sequence dimension (dim=1)\n",
    "final_embedding = torch.mean(token_embeddings, dim=1).squeeze()\n",
    "print(\"Final embedding shape:\", final_embedding.shape)\n",
    "\n",
    "# Convert to numpy array for easier handling\n",
    "embedding = final_embedding.cpu().numpy()\n",
    "print(\"Embedding numpy shape:\", embedding.shape)\n",
    "print(\"First 5 values:\", embedding[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bf6c51-e1d1-420d-81a9-44e47057130b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
