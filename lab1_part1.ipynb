{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c09cde9",
   "metadata": {},
   "source": [
    "# Lab 1: Building a Basic RAG System\n",
    "\n",
    "In this lab, we'll create a simple Retrieval Augmented Generation (RAG) system using PyTorch and Hugging Face models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c862768",
   "metadata": {},
   "source": [
    "## Setup\n",
    "First, let's import the necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cffec9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5defa09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third-party imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b49dbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available and set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4c7219",
   "metadata": {},
   "source": [
    "## Download and Load Language Model\n",
    "\n",
    "We'll use a pre-trained language model from Hugging Face for generating embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de07300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define which model to use - we'll use a small but effective model\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "print(f\"We'll use the model: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41738165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer - this converts text to tokens the model can understand\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(f\"Tokenizer loaded with vocabulary size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35d56bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how the tokenizer works with a simple example\n",
    "example_text = \"Hello, this is a sample text for our RAG system!\"\n",
    "tokens = tokenizer(example_text)\n",
    "print(\"Input text:\", example_text)\n",
    "print(\"Token IDs:\", tokens[\"input_ids\"])\n",
    "print(\"Decoded tokens:\", tokenizer.convert_ids_to_tokens(tokens[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fece451e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now load the actual model\n",
    "model = AutoModel.from_pretrained(model_name).to(device)\n",
    "print(f\"Model loaded successfully with {sum(p.numel() for p in model.parameters())} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ddc01c",
   "metadata": {},
   "source": [
    "## Generate Embeddings\n",
    "\n",
    "Now we'll see how to generate embeddings for text. Embeddings are vector representations that capture semantic meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f57628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, prepare text for the model by tokenizing it\n",
    "text_to_embed = \"This is a sample text to demonstrate embedding generation.\"\n",
    "encoded_input = tokenizer(text_to_embed, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "print(\"Encoded input shape:\")\n",
    "for key, value in encoded_input.items():\n",
    "    print(f\"  {key}: {value.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5276e6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the encoded input through the model\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "# Look at the model output\n",
    "print(\"Model output keys:\", model_output.keys())\n",
    "print(\"Last hidden state shape:\", model_output.last_hidden_state.shape)\n",
    "# This is a 3D tensor: [batch_size, sequence_length, hidden_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72085266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get a single vector for the entire text, we'll use mean pooling\n",
    "# Since we're only processing a single sentence without batching,\n",
    "# we can simply take the mean of the token embeddings\n",
    "token_embeddings = model_output.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b744e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean across the sequence dimension (dim=1)\n",
    "final_embedding = torch.mean(token_embeddings, dim=1).squeeze()\n",
    "print(\"Final embedding shape:\", final_embedding.shape)\n",
    "\n",
    "# Convert to numpy array for easier handling\n",
    "embedding = final_embedding.cpu().numpy()\n",
    "print(\"Embedding numpy shape:\", embedding.shape)\n",
    "print(\"First 5 values:\", embedding[:5])"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
