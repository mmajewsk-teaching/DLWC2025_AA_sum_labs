{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4eb01377",
   "metadata": {},
   "source": [
    "# Lab 1: Building a Basic RAG System\n",
    "\n",
    "In this lab, we'll create a simple Retrieval Augmented Generation (RAG) system using PyTorch and Hugging Face models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180b198a",
   "metadata": {},
   "source": [
    "## Setup\n",
    "First, let's import the necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12466b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038b1df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third-party imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0925131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available and set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96492d96",
   "metadata": {},
   "source": [
    "## Download and Load Language Model\n",
    "\n",
    "We'll use a pre-trained language model from Hugging Face for generating embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f49829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define which model to use - we'll use a small but effective model\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "print(f\"We'll use the model: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd498017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer - this converts text to tokens the model can understand\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(f\"Tokenizer loaded with vocabulary size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0aa2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how the tokenizer works with a simple example\n",
    "example_text = \"Hello, this is a sample text for our RAG system!\"\n",
    "tokens = tokenizer(example_text)\n",
    "print(\"Input text:\", example_text)\n",
    "print(\"Token IDs:\", tokens[\"input_ids\"])\n",
    "print(\"Decoded tokens:\", tokenizer.convert_ids_to_tokens(tokens[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1ba943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now load the actual model\n",
    "model = AutoModel.from_pretrained(model_name).to(device)\n",
    "print(f\"Model loaded successfully with {sum(p.numel() for p in model.parameters())} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e09106",
   "metadata": {},
   "source": [
    "## Generate Embeddings\n",
    "\n",
    "Now we'll see how to generate embeddings for text. Embeddings are vector representations that capture semantic meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7767908a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, prepare text for the model by tokenizing it\n",
    "text_to_embed = \"This is a sample text to demonstrate embedding generation.\"\n",
    "encoded_input = tokenizer(text_to_embed, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "print(\"Encoded input shape:\")\n",
    "for key, value in encoded_input.items():\n",
    "    print(f\"  {key}: {value.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc8acfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the encoded input through the model\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "# Look at the model output\n",
    "print(\"Model output keys:\", model_output.keys())\n",
    "print(\"Last hidden state shape:\", model_output.last_hidden_state.shape)\n",
    "# This is a 3D tensor: [batch_size, sequence_length, hidden_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77265c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get a single vector for the entire text, we'll use mean pooling\n",
    "# First, get the token embeddings and attention mask\n",
    "token_embeddings = model_output.last_hidden_state\n",
    "attention_mask = encoded_input[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ff4af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform mean pooling - expand attention mask to same dimension as embeddings\n",
    "input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "print(\"Expanded mask shape:\", input_mask_expanded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba555bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum the embeddings, applying the attention mask\n",
    "# The mask ensures we only consider actual tokens, not padding\n",
    "sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "print(\"Sum embeddings shape:\", sum_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3339b63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize by the sum of the mask\n",
    "sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "print(\"Sum mask shape:\", sum_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372f9715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide to get the mean\n",
    "final_embedding = (sum_embeddings / sum_mask).squeeze()\n",
    "print(\"Final embedding shape:\", final_embedding.shape)\n",
    "\n",
    "# Convert to numpy array for easier handling\n",
    "embedding = final_embedding.cpu().numpy()\n",
    "print(\"Embedding numpy shape:\", embedding.shape)\n",
    "print(\"First 5 values:\", embedding[:5])"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
